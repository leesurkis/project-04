{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv, re\n",
    "import nltk\n",
    "import string\n",
    "from html.parser import HTMLParser\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from gensim import corpora, models, similarities, matutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('tweet_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leesurkis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(df.stemmed)\n",
    "count_vecs = pd.DataFrame(X.toarray(), index=df.stemmed, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 6927)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aateam</th>\n",
       "      <th>abbys</th>\n",
       "      <th>aber</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abhijit</th>\n",
       "      <th>ability</th>\n",
       "      <th>abla</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeth</th>\n",
       "      <th>zika</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemmed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fuck heck move fridge knock landlord door angry mad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian uber driver just call word wasnt move vehicle jump disgust</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask parcel deliver pick store address fume poorcustomerservice</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whichever butt wipe pull alarm davis sound asleep piss angry upset tire sad tire hangry</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dont join phone talk rude take money acc willynilly fume</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    aaa  aaron  aateam  abbys  \\\n",
       "stemmed                                                                         \n",
       "fuck heck move fridge knock landlord door angry...    0      0       0      0   \n",
       "indian uber driver just call word wasnt move ve...    0      0       0      0   \n",
       "ask parcel deliver pick store address fume poor...    0      0       0      0   \n",
       "whichever butt wipe pull alarm davis sound asle...    0      0       0      0   \n",
       "dont join phone talk rude take money acc willyn...    0      0       0      0   \n",
       "\n",
       "                                                    aber  aberdeen  abhijit  \\\n",
       "stemmed                                                                       \n",
       "fuck heck move fridge knock landlord door angry...     0         0        0   \n",
       "indian uber driver just call word wasnt move ve...     0         0        0   \n",
       "ask parcel deliver pick store address fume poor...     0         0        0   \n",
       "whichever butt wipe pull alarm davis sound asle...     0         0        0   \n",
       "dont join phone talk rude take money acc willyn...     0         0        0   \n",
       "\n",
       "                                                    ability  abla  able  ...  \\\n",
       "stemmed                                                                  ...   \n",
       "fuck heck move fridge knock landlord door angry...        0     0     0  ...   \n",
       "indian uber driver just call word wasnt move ve...        0     0     0  ...   \n",
       "ask parcel deliver pick store address fume poor...        0     0     0  ...   \n",
       "whichever butt wipe pull alarm davis sound asle...        0     0     0  ...   \n",
       "dont join phone talk rude take money acc willyn...        0     0     0  ...   \n",
       "\n",
       "                                                    zero  zeth  zika  zilch  \\\n",
       "stemmed                                                                       \n",
       "fuck heck move fridge knock landlord door angry...     0     0     0      0   \n",
       "indian uber driver just call word wasnt move ve...     0     0     0      0   \n",
       "ask parcel deliver pick store address fume poor...     0     0     0      0   \n",
       "whichever butt wipe pull alarm davis sound asle...     0     0     0      0   \n",
       "dont join phone talk rude take money acc willyn...     0     0     0      0   \n",
       "\n",
       "                                                    zionist  zombie  zombies  \\\n",
       "stemmed                                                                        \n",
       "fuck heck move fridge knock landlord door angry...        0       0        0   \n",
       "indian uber driver just call word wasnt move ve...        0       0        0   \n",
       "ask parcel deliver pick store address fume poor...        0       0        0   \n",
       "whichever butt wipe pull alarm davis sound asle...        0       0        0   \n",
       "dont join phone talk rude take money acc willyn...        0       0        0   \n",
       "\n",
       "                                                    zone  zoolander  zumba  \n",
       "stemmed                                                                     \n",
       "fuck heck move fridge knock landlord door angry...     0          0      0  \n",
       "indian uber driver just call word wasnt move ve...     0          0      0  \n",
       "ask parcel deliver pick store address fume poor...     0          0      0  \n",
       "whichever butt wipe pull alarm davis sound asle...     0          0      0  \n",
       "dont join phone talk rude take money acc willyn...     0          0      0  \n",
       "\n",
       "[5 rows x 6927 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vecs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "\n",
    "X = word_vectorizer.fit_transform(df.stemmed)\n",
    "\n",
    "# char_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "#     analyzer='char',\n",
    "#     ngram_range=(1, 6),\n",
    "#     max_features=30000)\n",
    "# char_vectorizer.fit(df.stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvec = TfidfVectorizer(ngram_range=(1,2), binary=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vecs = pd.DataFrame(X.toarray(), index=df.stemmed, columns=word_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aateam</th>\n",
       "      <th>abbys</th>\n",
       "      <th>aber</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abhijit</th>\n",
       "      <th>ability</th>\n",
       "      <th>abla</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeth</th>\n",
       "      <th>zika</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemmed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fuck heck move fridge knock landlord door angry mad</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian uber driver just call word wasnt move vehicle jump disgust</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask parcel deliver pick store address fume poorcustomerservice</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whichever butt wipe pull alarm davis sound asleep piss angry upset tire sad tire hangry</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dont join phone talk rude take money acc willynilly fume</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6957 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    aaa  aaron  aateam  abbys  \\\n",
       "stemmed                                                                         \n",
       "fuck heck move fridge knock landlord door angry...  0.0    0.0     0.0    0.0   \n",
       "indian uber driver just call word wasnt move ve...  0.0    0.0     0.0    0.0   \n",
       "ask parcel deliver pick store address fume poor...  0.0    0.0     0.0    0.0   \n",
       "whichever butt wipe pull alarm davis sound asle...  0.0    0.0     0.0    0.0   \n",
       "dont join phone talk rude take money acc willyn...  0.0    0.0     0.0    0.0   \n",
       "\n",
       "                                                    aber  aberdeen  abhijit  \\\n",
       "stemmed                                                                       \n",
       "fuck heck move fridge knock landlord door angry...   0.0       0.0      0.0   \n",
       "indian uber driver just call word wasnt move ve...   0.0       0.0      0.0   \n",
       "ask parcel deliver pick store address fume poor...   0.0       0.0      0.0   \n",
       "whichever butt wipe pull alarm davis sound asle...   0.0       0.0      0.0   \n",
       "dont join phone talk rude take money acc willyn...   0.0       0.0      0.0   \n",
       "\n",
       "                                                    ability  abla  able  ...  \\\n",
       "stemmed                                                                  ...   \n",
       "fuck heck move fridge knock landlord door angry...      0.0   0.0   0.0  ...   \n",
       "indian uber driver just call word wasnt move ve...      0.0   0.0   0.0  ...   \n",
       "ask parcel deliver pick store address fume poor...      0.0   0.0   0.0  ...   \n",
       "whichever butt wipe pull alarm davis sound asle...      0.0   0.0   0.0  ...   \n",
       "dont join phone talk rude take money acc willyn...      0.0   0.0   0.0  ...   \n",
       "\n",
       "                                                    zero  zeth  zika  zilch  \\\n",
       "stemmed                                                                       \n",
       "fuck heck move fridge knock landlord door angry...   0.0   0.0   0.0    0.0   \n",
       "indian uber driver just call word wasnt move ve...   0.0   0.0   0.0    0.0   \n",
       "ask parcel deliver pick store address fume poor...   0.0   0.0   0.0    0.0   \n",
       "whichever butt wipe pull alarm davis sound asle...   0.0   0.0   0.0    0.0   \n",
       "dont join phone talk rude take money acc willyn...   0.0   0.0   0.0    0.0   \n",
       "\n",
       "                                                    zionist  zombie  zombies  \\\n",
       "stemmed                                                                        \n",
       "fuck heck move fridge knock landlord door angry...      0.0     0.0      0.0   \n",
       "indian uber driver just call word wasnt move ve...      0.0     0.0      0.0   \n",
       "ask parcel deliver pick store address fume poor...      0.0     0.0      0.0   \n",
       "whichever butt wipe pull alarm davis sound asle...      0.0     0.0      0.0   \n",
       "dont join phone talk rude take money acc willyn...      0.0     0.0      0.0   \n",
       "\n",
       "                                                    zone  zoolander  zumba  \n",
       "stemmed                                                                     \n",
       "fuck heck move fridge knock landlord door angry...   0.0        0.0    0.0  \n",
       "indian uber driver just call word wasnt move ve...   0.0        0.0    0.0  \n",
       "ask parcel deliver pick store address fume poor...   0.0        0.0    0.0  \n",
       "whichever butt wipe pull alarm davis sound asle...   0.0        0.0    0.0  \n",
       "dont join phone talk rude take money acc willyn...   0.0        0.0    0.0  \n",
       "\n",
       "[5 rows x 6957 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vecs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00885203, 0.01269675, 0.0100346 , 0.00874681])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(4)\n",
    "doc_topic = lsa.fit_transform(count_vecs)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aateam</th>\n",
       "      <th>abbys</th>\n",
       "      <th>aber</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abhijit</th>\n",
       "      <th>ability</th>\n",
       "      <th>abla</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeth</th>\n",
       "      <th>zika</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 6927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaa  aaron  aateam  abbys  aber  aberdeen  abhijit  ability  \\\n",
       "component_1  0.0    0.0     0.0    0.0   0.0     0.000      0.0    0.003   \n",
       "component_2 -0.0   -0.0    -0.0   -0.0  -0.0    -0.000     -0.0   -0.001   \n",
       "component_3  0.0    0.0     0.0   -0.0   0.0     0.000     -0.0   -0.001   \n",
       "component_4  0.0    0.0    -0.0    0.0   0.0     0.001      0.0    0.000   \n",
       "\n",
       "              abla   able  ...   zero  zeth  zika  zilch  zionist  zombie  \\\n",
       "component_1  0.002  0.002  ...  0.005   0.0   0.0    0.0      0.0   0.001   \n",
       "component_2 -0.000  0.001  ... -0.001  -0.0  -0.0   -0.0     -0.0  -0.000   \n",
       "component_3  0.003  0.003  ... -0.005   0.0  -0.0   -0.0      0.0  -0.000   \n",
       "component_4 -0.001 -0.001  ...  0.003   0.0   0.0    0.0     -0.0   0.001   \n",
       "\n",
       "             zombies  zone  zoolander  zumba  \n",
       "component_1    0.000   0.0      0.001    0.0  \n",
       "component_2    0.001  -0.0      0.003   -0.0  \n",
       "component_3    0.000  -0.0     -0.000   -0.0  \n",
       "component_4    0.000   0.0      0.000    0.0  \n",
       "\n",
       "[4 rows x 6927 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\",\"component_3\", \"component_4\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "just, like, dont, make, want, know, think, love, watch, say, people, time, feel, good, lively, need, day, really, happy, look\n",
      "\n",
      "Topic  1\n",
      "lively, watch, amaze, broadcast, musically, musicallyjh, follow, youve, series, long, horror, wonderful, season, glee, advise, gotham, fallenskies, syfy, jericho, heroes\n",
      "\n",
      "Topic  2\n",
      "like, make, look, feel, say, today, guy, smile, bad, old, sound, better, bully, day, rabid, polish, horrible, work, big, ask\n",
      "\n",
      "Topic  3\n",
      "dont, know, think, make, let, love, smile, people, day, youre, happy, fear, good, change, thank, things, worry, sad, world, expect\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, cv.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â LSA on TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01462442, 0.0027854 , 0.00374852, 0.00361182])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(4)\n",
    "doc_topic = lsa.fit_transform(tfidf_vecs)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aateam</th>\n",
       "      <th>abbys</th>\n",
       "      <th>aber</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abhijit</th>\n",
       "      <th>ability</th>\n",
       "      <th>abla</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeth</th>\n",
       "      <th>zika</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 6957 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaa  aaron  aateam  abbys  aber  aberdeen  abhijit  ability  \\\n",
       "component_1  0.0  0.000   0.000  0.000   0.0     0.000      0.0    0.000   \n",
       "component_2  0.0  0.001   0.001  0.001   0.0     0.002      0.0    0.006   \n",
       "component_3 -0.0  0.000  -0.003 -0.000  -0.0     0.001     -0.0   -0.003   \n",
       "component_4 -0.0  0.000  -0.001  0.000  -0.0     0.002     -0.0    0.010   \n",
       "\n",
       "              abla   able  ...   zero  zeth  zika  zilch  zionist  zombie  \\\n",
       "component_1  0.000  0.000  ...  0.000   0.0   0.0  0.000      0.0   0.000   \n",
       "component_2  0.002  0.003  ...  0.005   0.0   0.0  0.001      0.0   0.003   \n",
       "component_3 -0.001 -0.002  ...  0.002  -0.0   0.0 -0.001     -0.0  -0.001   \n",
       "component_4  0.002  0.002  ... -0.005   0.0   0.0  0.000     -0.0  -0.000   \n",
       "\n",
       "             zombies   zone  zoolander  zumba  \n",
       "component_1      0.0  0.000      0.002  0.000  \n",
       "component_2      0.0  0.001      0.001  0.000  \n",
       "component_3     -0.0 -0.001     -0.000 -0.001  \n",
       "component_4      0.0  0.000     -0.002  0.001  \n",
       "\n",
       "[4 rows x 6957 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\",\"component_3\", \"component_4\"],\n",
    "             columns = word_vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "lively, broadcast, musically, amaze, watch, musicallyjh, just, like, dont, do, time, listen, lovely, get, horror, follow, hilarious, thank, wait, start\n",
      "\n",
      "Topic  1\n",
      "dont, just, like, make, think, love, know, want, feel, get, do, good, people, day, go, happy, let, say, smile, start\n",
      "\n",
      "Topic  2\n",
      "dont, know, think, let, love, worry, bitter, youre, wanna, leave, smile, sad, provoke, make, shy, patronus, want, purpose, better, fear\n",
      "\n",
      "Topic  3\n",
      "make, happy, day, smile, good, love, let, youre, optimism, thank, birthday, today, look, quotesoup, decision, morning, joyful, bless, bday, angry\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, word_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(4)\n",
    "doc_topic = nmf_model.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aateam</th>\n",
       "      <th>abbys</th>\n",
       "      <th>aber</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abhijit</th>\n",
       "      <th>ability</th>\n",
       "      <th>abla</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeth</th>\n",
       "      <th>zika</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 6927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaa  aaron  aateam  abbys  aber  aberdeen  abhijit  ability  \\\n",
       "component_1  0.0  0.000     0.0  0.001   0.0     0.000      0.0    0.013   \n",
       "component_2  0.0  0.000     0.0  0.000   0.0     0.000      0.0    0.000   \n",
       "component_3  0.0  0.000     0.0  0.000   0.0     0.000      0.0    0.001   \n",
       "component_4  0.0  0.001     0.0  0.002   0.0     0.004      0.0    0.009   \n",
       "\n",
       "              abla   able  ...   zero   zeth   zika  zilch  zionist  zombie  \\\n",
       "component_1  0.000  0.000  ...  0.027  0.000  0.000  0.000      0.0   0.002   \n",
       "component_2  0.000  0.006  ...  0.000  0.000  0.000  0.000      0.0   0.000   \n",
       "component_3  0.012  0.013  ...  0.000  0.000  0.000  0.000      0.0   0.000   \n",
       "component_4  0.002  0.002  ...  0.018  0.001  0.001  0.002      0.0   0.005   \n",
       "\n",
       "             zombies   zone  zoolander  zumba  \n",
       "component_1    0.000  0.000      0.000  0.000  \n",
       "component_2    0.005  0.000      0.012  0.000  \n",
       "component_3    0.000  0.000      0.000  0.000  \n",
       "component_4    0.000  0.003      0.000  0.001  \n",
       "\n",
       "[4 rows x 6927 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\",\"component_3\", \"component_4\"],\n",
    "             columns = cv.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "just, want, time, say, really, way, love, need, wish, didnt, start, tell, little, live, lol\n",
      "\n",
      "Topic  1\n",
      "lively, watch, amaze, broadcast, musically, time, musicallyjh, follow, wait, horror, youve, long, hilarious, series, season\n",
      "\n",
      "Topic  2\n",
      "like, feel, look, make, say, today, bad, guy, old, fuck, right, sound, work, bully, laugh\n",
      "\n",
      "Topic  3\n",
      "dont, make, know, think, love, let, people, day, smile, good, youre, happy, want, time, fear\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, cv.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aateam</th>\n",
       "      <th>abbys</th>\n",
       "      <th>aber</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abhijit</th>\n",
       "      <th>ability</th>\n",
       "      <th>abla</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeth</th>\n",
       "      <th>zika</th>\n",
       "      <th>zilch</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6957 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               aaa  aaron  aateam  abbys   aber  aberdeen  abhijit  ability  \\\n",
       "component_1  0.000  0.000   0.000  0.000  0.000     0.000    0.000    0.000   \n",
       "component_2  0.001  0.001   0.004  0.002  0.001     0.000    0.001    0.006   \n",
       "component_3  0.000  0.001   0.000  0.000  0.000     0.005    0.000    0.000   \n",
       "component_4  0.000  0.001   0.000  0.001  0.000     0.002    0.000    0.021   \n",
       "component_5  0.000  0.000   0.003  0.000  0.000     0.000    0.000    0.000   \n",
       "\n",
       "              abla   able  ...   zero   zeth  zika  zilch  zionist  zombie  \\\n",
       "component_1  0.000  0.001  ...  0.000  0.000   0.0  0.000      0.0   0.000   \n",
       "component_2  0.000  0.002  ...  0.018  0.001   0.0  0.003      0.0   0.006   \n",
       "component_3  0.000  0.000  ...  0.008  0.000   0.0  0.000      0.0   0.000   \n",
       "component_4  0.004  0.004  ...  0.000  0.001   0.0  0.001      0.0   0.003   \n",
       "component_5  0.005  0.006  ...  0.000  0.000   0.0  0.000      0.0   0.001   \n",
       "\n",
       "             zombies   zone  zoolander  zumba  \n",
       "component_1    0.001  0.000      0.006  0.000  \n",
       "component_2    0.000  0.002      0.003  0.000  \n",
       "component_3    0.000  0.000      0.000  0.000  \n",
       "component_4    0.001  0.001      0.000  0.002  \n",
       "component_5    0.000  0.000      0.000  0.000  \n",
       "\n",
       "[5 rows x 6957 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\",\"component_3\", \"component_4\", \"component_5\"],\n",
    "             columns = word_vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "lively, broadcast, musically, amaze, watch, musicallyjh, listen, lovely, follow, horror, blake, pity, hilarious, buffnpolish, shower\n",
      "\n",
      "Topic  1\n",
      "just, get, go, want, time, say, do, people, need, really, start, lose, fuck, didnt, today\n",
      "\n",
      "Topic  2\n",
      "dont, know, think, let, worry, leave, bitter, sad, want, wanna, better, youre, tell, anger, shy\n",
      "\n",
      "Topic  3\n",
      "make, love, happy, day, smile, good, youre, thank, let, start, optimism, today, birthday, joyful, morning\n",
      "\n",
      "Topic  4\n",
      "like, feel, look, do, offend, depression, right, bad, worthless, optimism, today, literally, guy, laugh, old\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, word_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on Count Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term-doc matrix, transpose so that terms are rows\n",
    "\n",
    "doc_word = cv.transform(count_vecs).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6917</th>\n",
       "      <th>6918</th>\n",
       "      <th>6919</th>\n",
       "      <th>6920</th>\n",
       "      <th>6921</th>\n",
       "      <th>6922</th>\n",
       "      <th>6923</th>\n",
       "      <th>6924</th>\n",
       "      <th>6925</th>\n",
       "      <th>6926</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aateam</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbys</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aber</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9     ...  6917  \\\n",
       "aaa        1     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "aaron      0     1     0     0     0     0     0     0     0     0  ...     0   \n",
       "aateam     0     0     1     0     0     0     0     0     0     0  ...     0   \n",
       "abbys      0     0     0     1     0     0     0     0     0     0  ...     0   \n",
       "aber       0     0     0     0     1     0     0     0     0     0  ...     0   \n",
       "\n",
       "        6918  6919  6920  6921  6922  6923  6924  6925  6926  \n",
       "aaa        0     0     0     0     0     0     0     0     0  \n",
       "aaron      0     0     0     0     0     0     0     0     0  \n",
       "aateam     0     0     0     0     0     0     0     0     0  \n",
       "abbys      0     0     0     0     0     0     0     0     0  \n",
       "aber       0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 6927 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_word.toarray(), cv.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6927, 6927)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts sparse matrix of counts to a gensim corpus \n",
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6927"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-21 12:49:35,019 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2019-05-21 12:49:35,020 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2019-05-21 12:49:35,022 : INFO : using serial LDA version on this node\n",
      "2019-05-21 12:49:35,027 : INFO : running online (multi-pass) LDA training, 3 topics, 5 passes over the supplied corpus of 6927 documents, updating model once every 2000 documents, evaluating perplexity every 6927 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-05-21 12:49:35,032 : INFO : PROGRESS: pass 0, at document #2000/6927\n",
      "2019-05-21 12:49:35,735 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:35,738 : INFO : topic #0 (0.333): 0.001*\"bulushi\" + 0.001*\"dfs\" + 0.001*\"customerservice\" + 0.001*\"bbcfootball\" + 0.001*\"admit\" + 0.001*\"express\" + 0.001*\"course\" + 0.001*\"apprehension\" + 0.001*\"binary\" + 0.001*\"bojack\"\n",
      "2019-05-21 12:49:35,739 : INFO : topic #1 (0.333): 0.001*\"airplane\" + 0.001*\"aww\" + 0.001*\"diss\" + 0.001*\"eish\" + 0.001*\"bull\" + 0.001*\"brotein\" + 0.001*\"classic\" + 0.001*\"bizhour\" + 0.001*\"damnit\" + 0.001*\"boltup\"\n",
      "2019-05-21 12:49:35,740 : INFO : topic #2 (0.333): 0.001*\"acounthe\" + 0.001*\"devoid\" + 0.001*\"bass\" + 0.001*\"argentina\" + 0.001*\"extra\" + 0.001*\"draft\" + 0.001*\"alwayss\" + 0.001*\"applications\" + 0.001*\"davinago\" + 0.001*\"bag\"\n",
      "2019-05-21 12:49:35,741 : INFO : topic diff=1.774848, rho=1.000000\n",
      "2019-05-21 12:49:35,747 : INFO : PROGRESS: pass 0, at document #4000/6927\n",
      "2019-05-21 12:49:37,014 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:37,017 : INFO : topic #0 (0.333): 0.000*\"nawaz\" + 0.000*\"manner\" + 0.000*\"learn\" + 0.000*\"mouth\" + 0.000*\"improve\" + 0.000*\"nbecause\" + 0.000*\"litigious\" + 0.000*\"hangry\" + 0.000*\"girl\" + 0.000*\"healthy\"\n",
      "2019-05-21 12:49:37,018 : INFO : topic #1 (0.333): 0.000*\"headers\" + 0.000*\"goanyone\" + 0.000*\"loc\" + 0.000*\"hit\" + 0.000*\"memorize\" + 0.000*\"mark\" + 0.000*\"inspire\" + 0.000*\"fearful\" + 0.000*\"flu\" + 0.000*\"genless\"\n",
      "2019-05-21 12:49:37,019 : INFO : topic #2 (0.333): 0.000*\"klitschko\" + 0.000*\"gud\" + 0.000*\"movies\" + 0.000*\"jaunty\" + 0.000*\"getsauced\" + 0.000*\"higher\" + 0.000*\"jon\" + 0.000*\"function\" + 0.000*\"kay\" + 0.000*\"foreign\"\n",
      "2019-05-21 12:49:37,020 : INFO : topic diff=1.142249, rho=0.707107\n",
      "2019-05-21 12:49:37,025 : INFO : PROGRESS: pass 0, at document #6000/6927\n",
      "2019-05-21 12:49:38,140 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:38,143 : INFO : topic #0 (0.333): 0.000*\"nawaz\" + 0.000*\"manner\" + 0.000*\"learn\" + 0.000*\"mouth\" + 0.000*\"improve\" + 0.000*\"nbecause\" + 0.000*\"litigious\" + 0.000*\"hangry\" + 0.000*\"girl\" + 0.000*\"healthy\"\n",
      "2019-05-21 12:49:38,144 : INFO : topic #1 (0.333): 0.000*\"parent\" + 0.000*\"strangers\" + 0.000*\"php\" + 0.000*\"shrug\" + 0.000*\"refugeesnninyourneighborhoodnotdc\" + 0.000*\"phenomanal\" + 0.000*\"surprisepeople\" + 0.000*\"roald\" + 0.000*\"nephalism\" + 0.000*\"nut\"\n",
      "2019-05-21 12:49:38,145 : INFO : topic #2 (0.333): 0.000*\"shitworldforourkids\" + 0.000*\"scent\" + 0.000*\"officersshould\" + 0.000*\"spark\" + 0.000*\"potentially\" + 0.000*\"nerve\" + 0.000*\"somber\" + 0.000*\"ounce\" + 0.000*\"photography\" + 0.000*\"sufficiently\"\n",
      "2019-05-21 12:49:38,146 : INFO : topic diff=1.055687, rho=0.577350\n",
      "2019-05-21 12:49:38,629 : INFO : -12.969 per-word bound, 8016.3 perplexity estimate based on a held-out corpus of 927 documents with 927 words\n",
      "2019-05-21 12:49:38,629 : INFO : PROGRESS: pass 0, at document #6927/6927\n",
      "2019-05-21 12:49:39,014 : INFO : merging changes from 927 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:39,017 : INFO : topic #0 (0.333): 0.001*\"wantogoback\" + 0.001*\"vitamin\" + 0.001*\"teacher\" + 0.001*\"weak\" + 0.001*\"writersblock\" + 0.001*\"user\" + 0.001*\"tomboy\" + 0.001*\"thirst\" + 0.001*\"twitch\" + 0.001*\"unprepared\"\n",
      "2019-05-21 12:49:39,017 : INFO : topic #1 (0.333): 0.000*\"parent\" + 0.000*\"strangers\" + 0.000*\"php\" + 0.000*\"shrug\" + 0.000*\"refugeesnninyourneighborhoodnotdc\" + 0.000*\"phenomanal\" + 0.000*\"surprisepeople\" + 0.000*\"roald\" + 0.000*\"nephalism\" + 0.000*\"nut\"\n",
      "2019-05-21 12:49:39,018 : INFO : topic #2 (0.333): 0.000*\"shitworldforourkids\" + 0.000*\"scent\" + 0.000*\"officersshould\" + 0.000*\"spark\" + 0.000*\"potentially\" + 0.000*\"nerve\" + 0.000*\"somber\" + 0.000*\"ounce\" + 0.000*\"photography\" + 0.000*\"sufficiently\"\n",
      "2019-05-21 12:49:39,019 : INFO : topic diff=0.791544, rho=0.500000\n",
      "2019-05-21 12:49:39,025 : INFO : PROGRESS: pass 1, at document #2000/6927\n",
      "2019-05-21 12:49:39,304 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:39,307 : INFO : topic #0 (0.333): 0.000*\"wantogoback\" + 0.000*\"teacher\" + 0.000*\"vitamin\" + 0.000*\"weak\" + 0.000*\"writersblock\" + 0.000*\"user\" + 0.000*\"tomboy\" + 0.000*\"thirst\" + 0.000*\"twitch\" + 0.000*\"unprepared\"\n",
      "2019-05-21 12:49:39,308 : INFO : topic #1 (0.333): 0.000*\"airplane\" + 0.000*\"aww\" + 0.000*\"diss\" + 0.000*\"eish\" + 0.000*\"bull\" + 0.000*\"brotein\" + 0.000*\"classic\" + 0.000*\"damnit\" + 0.000*\"arrive\" + 0.000*\"bizhour\"\n",
      "2019-05-21 12:49:39,309 : INFO : topic #2 (0.333): 0.000*\"acounthe\" + 0.000*\"devoid\" + 0.000*\"bass\" + 0.000*\"extra\" + 0.000*\"argentina\" + 0.000*\"draft\" + 0.000*\"applications\" + 0.000*\"alwayss\" + 0.000*\"davinago\" + 0.000*\"behave\"\n",
      "2019-05-21 12:49:39,309 : INFO : topic diff=0.563259, rho=0.427823\n",
      "2019-05-21 12:49:39,316 : INFO : PROGRESS: pass 1, at document #4000/6927\n",
      "2019-05-21 12:49:39,662 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:39,665 : INFO : topic #0 (0.333): 0.000*\"nawaz\" + 0.000*\"improve\" + 0.000*\"neals\" + 0.000*\"nbecause\" + 0.000*\"ily\" + 0.000*\"internationaldayofpeace\" + 0.000*\"manner\" + 0.000*\"meaindia\" + 0.000*\"marniemccormack\" + 0.000*\"losetheyre\"\n",
      "2019-05-21 12:49:39,666 : INFO : topic #1 (0.333): 0.001*\"headers\" + 0.001*\"goanyone\" + 0.001*\"loc\" + 0.001*\"hit\" + 0.001*\"memorize\" + 0.001*\"mark\" + 0.001*\"inspire\" + 0.001*\"fearful\" + 0.001*\"flu\" + 0.001*\"genless\"\n",
      "2019-05-21 12:49:39,667 : INFO : topic #2 (0.333): 0.000*\"klitschko\" + 0.000*\"gud\" + 0.000*\"movies\" + 0.000*\"jaunty\" + 0.000*\"getsauced\" + 0.000*\"higher\" + 0.000*\"jon\" + 0.000*\"function\" + 0.000*\"kay\" + 0.000*\"foreign\"\n",
      "2019-05-21 12:49:39,668 : INFO : topic diff=0.497048, rho=0.427823\n",
      "2019-05-21 12:49:39,674 : INFO : PROGRESS: pass 1, at document #6000/6927\n",
      "2019-05-21 12:49:39,960 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:39,963 : INFO : topic #0 (0.333): 0.000*\"nawaz\" + 0.000*\"improve\" + 0.000*\"neals\" + 0.000*\"nbecause\" + 0.000*\"ily\" + 0.000*\"internationaldayofpeace\" + 0.000*\"manner\" + 0.000*\"meaindia\" + 0.000*\"marniemccormack\" + 0.000*\"losetheyre\"\n",
      "2019-05-21 12:49:39,964 : INFO : topic #1 (0.333): 0.000*\"php\" + 0.000*\"raw\" + 0.000*\"strangers\" + 0.000*\"ruffle\" + 0.000*\"sullen\" + 0.000*\"shrug\" + 0.000*\"phenomanal\" + 0.000*\"surprisepeople\" + 0.000*\"purpose\" + 0.000*\"roald\"\n",
      "2019-05-21 12:49:39,965 : INFO : topic #2 (0.333): 0.000*\"nsfw\" + 0.000*\"nhi\" + 0.000*\"officersshould\" + 0.000*\"playoffs\" + 0.000*\"patroons\" + 0.000*\"solid\" + 0.000*\"putin\" + 0.000*\"raid\" + 0.000*\"suffocate\" + 0.000*\"sufficiently\"\n",
      "2019-05-21 12:49:39,965 : INFO : topic diff=0.520748, rho=0.427823\n",
      "2019-05-21 12:49:40,139 : INFO : -10.897 per-word bound, 1906.4 perplexity estimate based on a held-out corpus of 927 documents with 927 words\n",
      "2019-05-21 12:49:40,140 : INFO : PROGRESS: pass 1, at document #6927/6927\n",
      "2019-05-21 12:49:40,255 : INFO : merging changes from 927 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:40,259 : INFO : topic #0 (0.333): 0.001*\"writersblock\" + 0.001*\"turnt\" + 0.001*\"worth\" + 0.001*\"vitamin\" + 0.001*\"teacher\" + 0.001*\"tgit\" + 0.001*\"tomboy\" + 0.001*\"tummy\" + 0.001*\"weak\" + 0.001*\"wantogoback\"\n",
      "2019-05-21 12:49:40,260 : INFO : topic #1 (0.333): 0.000*\"php\" + 0.000*\"raw\" + 0.000*\"strangers\" + 0.000*\"ruffle\" + 0.000*\"sullen\" + 0.000*\"shrug\" + 0.000*\"phenomanal\" + 0.000*\"surprisepeople\" + 0.000*\"purpose\" + 0.000*\"roald\"\n",
      "2019-05-21 12:49:40,261 : INFO : topic #2 (0.333): 0.000*\"nsfw\" + 0.000*\"nhi\" + 0.000*\"officersshould\" + 0.000*\"playoffs\" + 0.000*\"patroons\" + 0.000*\"solid\" + 0.000*\"putin\" + 0.000*\"raid\" + 0.000*\"suffocate\" + 0.000*\"saddenedno\"\n",
      "2019-05-21 12:49:40,261 : INFO : topic diff=0.369479, rho=0.427823\n",
      "2019-05-21 12:49:40,267 : INFO : PROGRESS: pass 2, at document #2000/6927\n",
      "2019-05-21 12:49:40,504 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:40,507 : INFO : topic #0 (0.333): 0.000*\"writersblock\" + 0.000*\"turnt\" + 0.000*\"worth\" + 0.000*\"vitamin\" + 0.000*\"teacher\" + 0.000*\"tgit\" + 0.000*\"weak\" + 0.000*\"tomboy\" + 0.000*\"tummy\" + 0.000*\"wantogoback\"\n",
      "2019-05-21 12:49:40,508 : INFO : topic #1 (0.333): 0.000*\"airplane\" + 0.000*\"aww\" + 0.000*\"diss\" + 0.000*\"eish\" + 0.000*\"bull\" + 0.000*\"brotein\" + 0.000*\"classic\" + 0.000*\"damnit\" + 0.000*\"arrive\" + 0.000*\"circle\"\n",
      "2019-05-21 12:49:40,509 : INFO : topic #2 (0.333): 0.000*\"acounthe\" + 0.000*\"devoid\" + 0.000*\"bass\" + 0.000*\"extra\" + 0.000*\"draft\" + 0.000*\"applications\" + 0.000*\"argentina\" + 0.000*\"behave\" + 0.000*\"decide\" + 0.000*\"endure\"\n",
      "2019-05-21 12:49:40,510 : INFO : topic diff=0.415886, rho=0.393338\n",
      "2019-05-21 12:49:40,557 : INFO : PROGRESS: pass 2, at document #4000/6927\n",
      "2019-05-21 12:49:40,786 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:40,789 : INFO : topic #0 (0.333): 0.000*\"ily\" + 0.000*\"gonzaga\" + 0.000*\"neals\" + 0.000*\"loose\" + 0.000*\"manner\" + 0.000*\"lyft\" + 0.000*\"mop\" + 0.000*\"nbecause\" + 0.000*\"nawaz\" + 0.000*\"job\"\n",
      "2019-05-21 12:49:40,790 : INFO : topic #1 (0.333): 0.001*\"headers\" + 0.001*\"goanyone\" + 0.001*\"loc\" + 0.001*\"hit\" + 0.001*\"memorize\" + 0.001*\"mark\" + 0.001*\"inspire\" + 0.001*\"fearful\" + 0.001*\"flu\" + 0.001*\"genless\"\n",
      "2019-05-21 12:49:40,791 : INFO : topic #2 (0.333): 0.001*\"klitschko\" + 0.001*\"gud\" + 0.001*\"movies\" + 0.001*\"jaunty\" + 0.001*\"getsauced\" + 0.001*\"higher\" + 0.001*\"jon\" + 0.001*\"function\" + 0.001*\"kay\" + 0.001*\"foreign\"\n",
      "2019-05-21 12:49:40,792 : INFO : topic diff=0.380307, rho=0.393338\n",
      "2019-05-21 12:49:40,797 : INFO : PROGRESS: pass 2, at document #6000/6927\n",
      "2019-05-21 12:49:40,998 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:41,001 : INFO : topic #0 (0.333): 0.000*\"ily\" + 0.000*\"gonzaga\" + 0.000*\"neals\" + 0.000*\"loose\" + 0.000*\"manner\" + 0.000*\"lyft\" + 0.000*\"mop\" + 0.000*\"nbecause\" + 0.000*\"job\" + 0.000*\"nawaz\"\n",
      "2019-05-21 12:49:41,002 : INFO : topic #1 (0.333): 0.000*\"php\" + 0.000*\"strangers\" + 0.000*\"shrug\" + 0.000*\"raw\" + 0.000*\"ruffle\" + 0.000*\"surprisepeople\" + 0.000*\"phenomanal\" + 0.000*\"sullen\" + 0.000*\"purpose\" + 0.000*\"nephalism\"\n",
      "2019-05-21 12:49:41,003 : INFO : topic #2 (0.333): 0.000*\"nsfw\" + 0.000*\"patroons\" + 0.000*\"playoffs\" + 0.000*\"putin\" + 0.000*\"sagin\" + 0.000*\"push\" + 0.000*\"strawberry\" + 0.000*\"nlast\" + 0.000*\"signal\" + 0.000*\"outfit\"\n",
      "2019-05-21 12:49:41,004 : INFO : topic diff=0.427991, rho=0.393338\n",
      "2019-05-21 12:49:41,162 : INFO : -10.662 per-word bound, 1620.4 perplexity estimate based on a held-out corpus of 927 documents with 927 words\n",
      "2019-05-21 12:49:41,163 : INFO : PROGRESS: pass 2, at document #6927/6927\n",
      "2019-05-21 12:49:41,246 : INFO : merging changes from 927 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:41,249 : INFO : topic #0 (0.333): 0.001*\"turnt\" + 0.001*\"writersblock\" + 0.001*\"weak\" + 0.001*\"tgit\" + 0.001*\"wantogoback\" + 0.001*\"vitamin\" + 0.001*\"teacher\" + 0.001*\"visible\" + 0.001*\"yasezayoni\" + 0.001*\"worth\"\n",
      "2019-05-21 12:49:41,250 : INFO : topic #1 (0.333): 0.000*\"php\" + 0.000*\"strangers\" + 0.000*\"raw\" + 0.000*\"shrug\" + 0.000*\"ruffle\" + 0.000*\"surprisepeople\" + 0.000*\"phenomanal\" + 0.000*\"sullen\" + 0.000*\"purpose\" + 0.000*\"nephalism\"\n",
      "2019-05-21 12:49:41,251 : INFO : topic #2 (0.333): 0.000*\"patroons\" + 0.000*\"nsfw\" + 0.000*\"playoffs\" + 0.000*\"sagin\" + 0.000*\"putin\" + 0.000*\"push\" + 0.000*\"strawberry\" + 0.000*\"nlast\" + 0.000*\"signal\" + 0.000*\"spark\"\n",
      "2019-05-21 12:49:41,252 : INFO : topic diff=0.340708, rho=0.393338\n",
      "2019-05-21 12:49:41,257 : INFO : PROGRESS: pass 3, at document #2000/6927\n",
      "2019-05-21 12:49:41,469 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:41,472 : INFO : topic #0 (0.333): 0.000*\"turnt\" + 0.000*\"writersblock\" + 0.000*\"weak\" + 0.000*\"tgit\" + 0.000*\"wantogoback\" + 0.000*\"vitamin\" + 0.000*\"teacher\" + 0.000*\"yasezayoni\" + 0.000*\"visible\" + 0.000*\"tomboy\"\n",
      "2019-05-21 12:49:41,473 : INFO : topic #1 (0.333): 0.000*\"airplane\" + 0.000*\"aww\" + 0.000*\"diss\" + 0.000*\"eish\" + 0.000*\"bull\" + 0.000*\"brotein\" + 0.000*\"arrive\" + 0.000*\"classic\" + 0.000*\"damnit\" + 0.000*\"circle\"\n",
      "2019-05-21 12:49:41,474 : INFO : topic #2 (0.333): 0.000*\"acounthe\" + 0.000*\"devoid\" + 0.000*\"bass\" + 0.000*\"extra\" + 0.000*\"draft\" + 0.000*\"applications\" + 0.000*\"argentina\" + 0.000*\"behave\" + 0.000*\"decide\" + 0.000*\"endure\"\n",
      "2019-05-21 12:49:41,475 : INFO : topic diff=0.352093, rho=0.366040\n",
      "2019-05-21 12:49:41,480 : INFO : PROGRESS: pass 3, at document #4000/6927\n",
      "2019-05-21 12:49:41,721 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:41,724 : INFO : topic #0 (0.333): 0.000*\"loose\" + 0.000*\"ily\" + 0.000*\"manner\" + 0.000*\"gonzaga\" + 0.000*\"mop\" + 0.000*\"nbecause\" + 0.000*\"losetheyre\" + 0.000*\"job\" + 0.000*\"holdin\" + 0.000*\"hephxho\"\n",
      "2019-05-21 12:49:41,725 : INFO : topic #1 (0.333): 0.001*\"headers\" + 0.001*\"goanyone\" + 0.001*\"loc\" + 0.001*\"hit\" + 0.001*\"memorize\" + 0.001*\"mark\" + 0.001*\"inspire\" + 0.001*\"fearful\" + 0.001*\"flu\" + 0.001*\"genless\"\n",
      "2019-05-21 12:49:41,726 : INFO : topic #2 (0.333): 0.001*\"klitschko\" + 0.001*\"gud\" + 0.001*\"movies\" + 0.001*\"jaunty\" + 0.001*\"getsauced\" + 0.001*\"higher\" + 0.001*\"jon\" + 0.001*\"kay\" + 0.001*\"function\" + 0.001*\"foreign\"\n",
      "2019-05-21 12:49:41,726 : INFO : topic diff=0.347067, rho=0.366040\n",
      "2019-05-21 12:49:41,732 : INFO : PROGRESS: pass 3, at document #6000/6927\n",
      "2019-05-21 12:49:41,943 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:41,946 : INFO : topic #0 (0.333): 0.000*\"loose\" + 0.000*\"ily\" + 0.000*\"manner\" + 0.000*\"gonzaga\" + 0.000*\"mop\" + 0.000*\"nbecause\" + 0.000*\"losetheyre\" + 0.000*\"job\" + 0.000*\"holdin\" + 0.000*\"hephxho\"\n",
      "2019-05-21 12:49:41,947 : INFO : topic #1 (0.333): 0.000*\"raw\" + 0.000*\"purpose\" + 0.000*\"roald\" + 0.000*\"rough\" + 0.000*\"nephalism\" + 0.000*\"shame\" + 0.000*\"peace\" + 0.000*\"openhouse\" + 0.000*\"rex\" + 0.000*\"pace\"\n",
      "2019-05-21 12:49:41,948 : INFO : topic #2 (0.333): 0.000*\"patroons\" + 0.000*\"nsfw\" + 0.000*\"strawberry\" + 0.000*\"signal\" + 0.000*\"shawty\" + 0.000*\"ounce\" + 0.000*\"slag\" + 0.000*\"paw\" + 0.000*\"potentially\" + 0.000*\"pajamas\"\n",
      "2019-05-21 12:49:41,949 : INFO : topic diff=0.378067, rho=0.366040\n",
      "2019-05-21 12:49:42,117 : INFO : -10.578 per-word bound, 1528.9 perplexity estimate based on a held-out corpus of 927 documents with 927 words\n",
      "2019-05-21 12:49:42,118 : INFO : PROGRESS: pass 3, at document #6927/6927\n",
      "2019-05-21 12:49:42,207 : INFO : merging changes from 927 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:42,210 : INFO : topic #0 (0.333): 0.001*\"writersblock\" + 0.001*\"turnt\" + 0.001*\"warrens\" + 0.001*\"wantogoback\" + 0.001*\"unending\" + 0.001*\"toomuchgoingon\" + 0.001*\"vitamin\" + 0.001*\"treatcustomersfairly\" + 0.001*\"worstcelebritycooks\" + 0.001*\"weak\"\n",
      "2019-05-21 12:49:42,210 : INFO : topic #1 (0.333): 0.000*\"purpose\" + 0.000*\"raw\" + 0.000*\"roald\" + 0.000*\"rough\" + 0.000*\"nephalism\" + 0.000*\"shame\" + 0.000*\"rex\" + 0.000*\"openhouse\" + 0.000*\"peace\" + 0.000*\"pace\"\n",
      "2019-05-21 12:49:42,211 : INFO : topic #2 (0.333): 0.000*\"patroons\" + 0.000*\"nsfw\" + 0.000*\"strawberry\" + 0.000*\"signal\" + 0.000*\"shawty\" + 0.000*\"ounce\" + 0.000*\"potentially\" + 0.000*\"saddenedno\" + 0.000*\"suffocate\" + 0.000*\"paw\"\n",
      "2019-05-21 12:49:42,212 : INFO : topic diff=0.317367, rho=0.366040\n",
      "2019-05-21 12:49:42,217 : INFO : PROGRESS: pass 4, at document #2000/6927\n",
      "2019-05-21 12:49:42,433 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:42,436 : INFO : topic #0 (0.333): 0.000*\"writersblock\" + 0.000*\"turnt\" + 0.000*\"warrens\" + 0.000*\"vitamin\" + 0.000*\"wantogoback\" + 0.000*\"worstcelebritycooks\" + 0.000*\"toomuchgoingon\" + 0.000*\"unending\" + 0.000*\"treatcustomersfairly\" + 0.000*\"youif\"\n",
      "2019-05-21 12:49:42,437 : INFO : topic #1 (0.333): 0.000*\"airplane\" + 0.000*\"aww\" + 0.000*\"diss\" + 0.000*\"eish\" + 0.000*\"bull\" + 0.000*\"bizhour\" + 0.000*\"airline\" + 0.000*\"brotein\" + 0.000*\"allah\" + 0.000*\"arrive\"\n",
      "2019-05-21 12:49:42,438 : INFO : topic #2 (0.333): 0.000*\"devoid\" + 0.000*\"bass\" + 0.000*\"acounthe\" + 0.000*\"extra\" + 0.000*\"applications\" + 0.000*\"draft\" + 0.000*\"argentina\" + 0.000*\"behave\" + 0.000*\"endure\" + 0.000*\"bloke\"\n",
      "2019-05-21 12:49:42,439 : INFO : topic diff=0.317411, rho=0.343736\n",
      "2019-05-21 12:49:42,445 : INFO : PROGRESS: pass 4, at document #4000/6927\n",
      "2019-05-21 12:49:42,672 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:42,675 : INFO : topic #0 (0.333): 0.000*\"ily\" + 0.000*\"mouth\" + 0.000*\"meaindia\" + 0.000*\"intensity\" + 0.000*\"inept\" + 0.000*\"gtgt\" + 0.000*\"gesture\" + 0.000*\"minus\" + 0.000*\"nbecause\" + 0.000*\"fullprice\"\n",
      "2019-05-21 12:49:42,676 : INFO : topic #1 (0.333): 0.001*\"headers\" + 0.001*\"goanyone\" + 0.001*\"loc\" + 0.001*\"hit\" + 0.001*\"memorize\" + 0.001*\"mark\" + 0.001*\"fearful\" + 0.001*\"inspire\" + 0.001*\"flu\" + 0.001*\"genless\"\n",
      "2019-05-21 12:49:42,677 : INFO : topic #2 (0.333): 0.000*\"klitschko\" + 0.000*\"gud\" + 0.000*\"movies\" + 0.000*\"jaunty\" + 0.000*\"getsauced\" + 0.000*\"higher\" + 0.000*\"jon\" + 0.000*\"kay\" + 0.000*\"function\" + 0.000*\"foreign\"\n",
      "2019-05-21 12:49:42,678 : INFO : topic diff=0.322240, rho=0.343736\n",
      "2019-05-21 12:49:42,683 : INFO : PROGRESS: pass 4, at document #6000/6927\n",
      "2019-05-21 12:49:42,875 : INFO : merging changes from 2000 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:42,879 : INFO : topic #0 (0.333): 0.000*\"ily\" + 0.000*\"mouth\" + 0.000*\"meaindia\" + 0.000*\"intensity\" + 0.000*\"inept\" + 0.000*\"gtgt\" + 0.000*\"nbecause\" + 0.000*\"minus\" + 0.000*\"gesture\" + 0.000*\"fullprice\"\n",
      "2019-05-21 12:49:42,880 : INFO : topic #1 (0.333): 0.000*\"photos\" + 0.000*\"phew\" + 0.000*\"saudi\" + 0.000*\"ruffle\" + 0.000*\"relate\" + 0.000*\"qualm\" + 0.000*\"sydney\" + 0.000*\"potential\" + 0.000*\"sleepingn\" + 0.000*\"paltry\"\n",
      "2019-05-21 12:49:42,880 : INFO : topic #2 (0.333): 0.000*\"strawberry\" + 0.000*\"ordunno\" + 0.000*\"shawty\" + 0.000*\"patroons\" + 0.000*\"somber\" + 0.000*\"superexcited\" + 0.000*\"rudely\" + 0.000*\"remark\" + 0.000*\"norffampton\" + 0.000*\"signal\"\n",
      "2019-05-21 12:49:42,881 : INFO : topic diff=0.346387, rho=0.343736\n",
      "2019-05-21 12:49:43,049 : INFO : -10.523 per-word bound, 1471.1 perplexity estimate based on a held-out corpus of 927 documents with 927 words\n",
      "2019-05-21 12:49:43,050 : INFO : PROGRESS: pass 4, at document #6927/6927\n",
      "2019-05-21 12:49:43,131 : INFO : merging changes from 927 documents into a model of 6927 documents\n",
      "2019-05-21 12:49:43,134 : INFO : topic #0 (0.333): 0.001*\"weak\" + 0.001*\"wespernalsontoo\" + 0.001*\"vast\" + 0.001*\"update\" + 0.001*\"treatcustomersfairly\" + 0.001*\"unacceptable\" + 0.001*\"younger\" + 0.001*\"unprepared\" + 0.001*\"violationnn\" + 0.001*\"youve\"\n",
      "2019-05-21 12:49:43,135 : INFO : topic #1 (0.333): 0.000*\"photos\" + 0.000*\"phew\" + 0.000*\"saudi\" + 0.000*\"ruffle\" + 0.000*\"relate\" + 0.000*\"qualm\" + 0.000*\"sydney\" + 0.000*\"potential\" + 0.000*\"sleepingn\" + 0.000*\"paltry\"\n",
      "2019-05-21 12:49:43,136 : INFO : topic #2 (0.333): 0.000*\"shawty\" + 0.000*\"strawberry\" + 0.000*\"ordunno\" + 0.000*\"patroons\" + 0.000*\"signal\" + 0.000*\"remark\" + 0.000*\"superexcited\" + 0.000*\"norffampton\" + 0.000*\"patheticservices\" + 0.000*\"rudely\"\n",
      "2019-05-21 12:49:43,137 : INFO : topic diff=0.295366, rho=0.343736\n"
     ]
    }
   ],
   "source": [
    "# 'fitting' the model\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-21 12:49:43,144 : INFO : topic #0 (0.333): 0.001*\"weak\" + 0.001*\"wespernalsontoo\" + 0.001*\"vast\" + 0.001*\"update\" + 0.001*\"treatcustomersfairly\" + 0.001*\"unacceptable\" + 0.001*\"younger\" + 0.001*\"unprepared\" + 0.001*\"violationnn\" + 0.001*\"youve\"\n",
      "2019-05-21 12:49:43,145 : INFO : topic #1 (0.333): 0.000*\"photos\" + 0.000*\"phew\" + 0.000*\"saudi\" + 0.000*\"ruffle\" + 0.000*\"relate\" + 0.000*\"qualm\" + 0.000*\"sydney\" + 0.000*\"potential\" + 0.000*\"sleepingn\" + 0.000*\"paltry\"\n",
      "2019-05-21 12:49:43,146 : INFO : topic #2 (0.333): 0.000*\"shawty\" + 0.000*\"strawberry\" + 0.000*\"ordunno\" + 0.000*\"patroons\" + 0.000*\"signal\" + 0.000*\"remark\" + 0.000*\"superexcited\" + 0.000*\"norffampton\" + 0.000*\"patheticservices\" + 0.000*\"rudely\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"weak\" + 0.001*\"wespernalsontoo\" + 0.001*\"vast\" + 0.001*\"update\" + 0.001*\"treatcustomersfairly\" + 0.001*\"unacceptable\" + 0.001*\"younger\" + 0.001*\"unprepared\" + 0.001*\"violationnn\" + 0.001*\"youve\"'),\n",
       " (1,\n",
       "  '0.000*\"photos\" + 0.000*\"phew\" + 0.000*\"saudi\" + 0.000*\"ruffle\" + 0.000*\"relate\" + 0.000*\"qualm\" + 0.000*\"sydney\" + 0.000*\"potential\" + 0.000*\"sleepingn\" + 0.000*\"paltry\"'),\n",
       " (2,\n",
       "  '0.000*\"shawty\" + 0.000*\"strawberry\" + 0.000*\"ordunno\" + 0.000*\"patroons\" + 0.000*\"signal\" + 0.000*\"remark\" + 0.000*\"superexcited\" + 0.000*\"norffampton\" + 0.000*\"patheticservices\" + 0.000*\"rudely\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1a2c7842e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_corpus = lda[corpus]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.6505064), (1, 0.17545708), (2, 0.17403653)],\n",
       " [(0, 0.16845603), (1, 0.6616514), (2, 0.16989258)],\n",
       " [(0, 0.1687343), (1, 0.17102896), (2, 0.6602368)],\n",
       " [(0, 0.6505465), (1, 0.17542997), (2, 0.17402358)],\n",
       " [(0, 0.16873457), (1, 0.17103206), (2, 0.6602334)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-15eda280a30e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "count_vecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
